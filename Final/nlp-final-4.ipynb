{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Task 4: Pretrained Language Models (BERT, GPT, T5)","metadata":{}},{"cell_type":"code","source":"pip install transformers datasets accelerate","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T11:32:26.766434Z","iopub.execute_input":"2025-05-04T11:32:26.766680Z","iopub.status.idle":"2025-05-04T11:33:42.138725Z","shell.execute_reply.started":"2025-05-04T11:32:26.766663Z","shell.execute_reply":"2025-05-04T11:33:42.137564Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.5.1+cu124)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2024.12.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# BERT","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\nfrom datasets import load_dataset\nimport torch\n\n#IMDB\ndataset = load_dataset(\"imdb\")\n\n#BERT\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T11:33:42.141057Z","iopub.execute_input":"2025-05-04T11:33:42.141908Z","iopub.status.idle":"2025-05-04T11:33:50.968681Z","shell.execute_reply.started":"2025-05-04T11:33:42.141863Z","shell.execute_reply":"2025-05-04T11:33:50.968137Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78edb66876ea4161ab0f3b9992a02f71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e946b44b7044acf92e306ad77d6de6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a149ea4dc7a411f8de91b31f9fea124"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05c2c9cd9b0040bcbe15408dc718289e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1908255b259c4614ac1a54389a8080ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c50997e8f6fb439595de9cf98ffe6e77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af5f969d657d4fcbb9110f4cbf0bc167"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09a04802ea8d49e9acfba449d0782249"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e23bd587f2544644b017f0186bbc3c03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca52f08331804540a098ff755d32b854"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef796d385a8b42dab2d086d324071cb4"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"def tokenize(example):\n    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n\ntokenized_ds = dataset.map(tokenize, batched=True)\ntokenized_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n\nmodel_BERT = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T11:33:50.969297Z","iopub.execute_input":"2025-05-04T11:33:50.969758Z","iopub.status.idle":"2025-05-04T11:40:06.260867Z","shell.execute_reply.started":"2025-05-04T11:33:50.969741Z","shell.execute_reply":"2025-05-04T11:40:06.260336Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4371d5709404439a9bbcbbbe3e21d4e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"675d3fc1c9894801a52ee200852754c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c40e02fdfb0847fc97b0ed661c081d4d"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54c1993b6b8c4bec9ea72d38e17ee483"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    return {\"accuracy\": accuracy_score(labels, preds)}\n\ntraining_args = TrainingArguments(\n    output_dir=\"./bert-imdb\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=4,\n    logging_dir=\"./logs\",\n    logging_steps=200,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    save_total_limit=1,\n    report_to=\"none\"\n)\n\n\ntrainer = Trainer(\n    model=model_BERT,\n    args=training_args,\n    train_dataset=tokenized_ds[\"train\"].shuffle(seed=42).select(range(5000)),  # ускорим обучение\n    eval_dataset=tokenized_ds[\"test\"].select(range(1000)),\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T11:40:06.262642Z","iopub.execute_input":"2025-05-04T11:40:06.263034Z","iopub.status.idle":"2025-05-04T11:51:20.170926Z","shell.execute_reply.started":"2025-05-04T11:40:06.262986Z","shell.execute_reply":"2025-05-04T11:51:20.170316Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1252' max='1252' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1252/1252 11:11, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.394400</td>\n      <td>0.425322</td>\n      <td>0.810000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.181100</td>\n      <td>0.552681</td>\n      <td>0.824000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.094300</td>\n      <td>0.618094</td>\n      <td>0.873000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.022300</td>\n      <td>0.754065</td>\n      <td>0.859000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1252, training_loss=0.1600307113827227, metrics={'train_runtime': 672.8547, 'train_samples_per_second': 29.724, 'train_steps_per_second': 1.861, 'total_flos': 2631110553600000.0, 'train_loss': 0.1600307113827227, 'epoch': 4.0})"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"eval_result = trainer.evaluate()\nprint(eval_result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T11:51:20.171687Z","iopub.execute_input":"2025-05-04T11:51:20.171967Z","iopub.status.idle":"2025-05-04T11:51:30.531123Z","shell.execute_reply.started":"2025-05-04T11:51:20.171949Z","shell.execute_reply":"2025-05-04T11:51:30.530555Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.6180936098098755, 'eval_accuracy': 0.873, 'eval_runtime': 10.3512, 'eval_samples_per_second': 96.607, 'eval_steps_per_second': 6.086, 'epoch': 4.0}\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# GPT","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\nimport torch\nfrom sklearn.metrics import accuracy_score\n\n# Загружаем датасет Shakespeare\ndataset = load_dataset(\"tiny_shakespeare\")\nprint(dataset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T22:35:51.741890Z","iopub.execute_input":"2025-05-06T22:35:51.742445Z","iopub.status.idle":"2025-05-06T22:38:11.840347Z","shell.execute_reply.started":"2025-05-06T22:35:51.742424Z","shell.execute_reply":"2025-05-06T22:38:11.839538Z"}},"outputs":[{"name":"stderr","text":"2025-05-06 22:35:57.280579: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746570957.509617      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746570957.572905      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/6.10k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f83aeb4ef28449cea7afd899f89ac569"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tiny_shakespeare.py:   0%|          | 0.00/3.73k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"411a9791183448f78909d652c11bc3eb"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for tiny_shakespeare contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/tiny_shakespeare.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.12M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5330978448434e1a89954652e9bcc2b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9889ff9df6214199b4f665f79e91dbe7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/1 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cb870755ad14903ae38a2a55385dedb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1ad25b0dc51467bb93a9404ca01df5c"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 1\n    })\n    validation: Dataset({\n        features: ['text'],\n        num_rows: 1\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 1\n    })\n})\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from transformers import GPT2Tokenizer\nfrom datasets import load_dataset\nimport itertools\n\n# === 1. Загрузка токенизатора ===\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\ntokenizer.pad_token = tokenizer.eos_token  # GPT2 не имеет pad_token по умолчанию\n\n# === 2. Загрузка датасета (предположим, уже есть текстовая колонка 'text') ===\n# dataset = load_dataset(\"path_to_shakespeare_dataset\")\n# Пример: dataset = load_dataset(\"tiny_shakespeare\")  # если есть такой\n\n# === 3. Токенизация ===\ndef tokenize_function(example):\n    return tokenizer(example[\"text\"], return_attention_mask=False)\n\ntokenized = dataset.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=[\"text\"]\n)\n\n# === 4. Удаление всех колонок кроме input_ids ===\ntokenized = tokenized.remove_columns(\n    [col for col in tokenized[\"train\"].column_names if col != \"input_ids\"]\n)\n\n# === 5. Группировка в блоки фиксированной длины ===\nblock_size = 250\n\ndef group_texts(examples):\n    concatenated = list(itertools.chain.from_iterable(examples[\"input_ids\"]))\n    total_length = (len(concatenated) // block_size) * block_size\n    input_ids = [\n        concatenated[i : i + block_size] \n        for i in range(0, total_length, block_size)\n    ]\n    return {\n        \"input_ids\": input_ids,\n        \"labels\": input_ids.copy(),  # language modeling\n    }\n\nlm_dataset = tokenized.map(group_texts, batched=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T22:38:11.841598Z","iopub.execute_input":"2025-05-06T22:38:11.841980Z","iopub.status.idle":"2025-05-06T22:38:16.645314Z","shell.execute_reply.started":"2025-05-06T22:38:11.841963Z","shell.execute_reply":"2025-05-06T22:38:16.644547Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e066a92f29549e5884505049e64d099"}},"metadata":{}},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (301966 > 1024). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fc9e554df3a43d3b3ffabe7bb881ae1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"693b3e963efc4f54bec36c865d23b67b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3672e9c49ea7431eafd1bd60d5fafdff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f6a1eb2beb4459e9a7cb310ae21a912"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4926a64b07c048db993ee04fb3dee04f"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"from transformers import GPT2LMHeadModel\n\nmodel_GPT = GPT2LMHeadModel.from_pretrained(\"gpt2\")\nmodel_GPT.resize_token_embeddings(len(tokenizer))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T22:38:16.646290Z","iopub.execute_input":"2025-05-06T22:38:16.646572Z","iopub.status.idle":"2025-05-06T22:38:19.619949Z","shell.execute_reply.started":"2025-05-06T22:38:16.646545Z","shell.execute_reply":"2025-05-06T22:38:19.619092Z"}},"outputs":[{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2991aaa971ba43d9ac533375ba95034e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bf93c21300249928f28f0945cf0736b"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"Embedding(50257, 768)"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./gpt2-shakespeare\",\n    overwrite_output_dir=True,\n    per_device_train_batch_size=2,\n    num_train_epochs=20,\n    save_strategy=\"no\",\n    logging_steps=200,\n    report_to=\"none\"\n)\n\n\nfrom transformers import Trainer\n\ntrainer = Trainer(\n    model=model_GPT,\n    args=training_args,\n    train_dataset=lm_dataset[\"train\"]\n)\n\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T22:38:19.622140Z","iopub.execute_input":"2025-05-06T22:38:19.622848Z","iopub.status.idle":"2025-05-06T22:58:33.122207Z","shell.execute_reply.started":"2025-05-06T22:38:19.622821Z","shell.execute_reply":"2025-05-06T22:58:33.121595Z"}},"outputs":[{"name":"stderr","text":"`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6040' max='6040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6040/6040 20:11, Epoch 20/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>1.887600</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.775500</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.710200</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>1.653000</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.651000</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>1.622400</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>1.562100</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>1.535600</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>1.515300</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.477700</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>1.471700</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>1.461600</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>1.420200</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>1.408800</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>1.405200</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>1.373800</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>1.355400</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>1.347300</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>1.332600</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>1.326100</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>1.290000</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>1.316600</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>1.287900</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>1.279400</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>1.266200</td>\n    </tr>\n    <tr>\n      <td>5200</td>\n      <td>1.257300</td>\n    </tr>\n    <tr>\n      <td>5400</td>\n      <td>1.241900</td>\n    </tr>\n    <tr>\n      <td>5600</td>\n      <td>1.257400</td>\n    </tr>\n    <tr>\n      <td>5800</td>\n      <td>1.250700</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>1.239800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=6040, training_loss=1.4315940073783824, metrics={'train_runtime': 1212.7101, 'train_samples_per_second': 19.906, 'train_steps_per_second': 4.981, 'total_flos': 3079877760000000.0, 'train_loss': 1.4315940073783824, 'epoch': 20.0})"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"from transformers import pipeline\n\ngenerator = pipeline(\"text-generation\", model=model_GPT, tokenizer=tokenizer)\n\nseeds = [\n    \"To be, or not to be\",\n    \"O Romeo, Romeo! wherefore art thou\",\n    \"The night is dark and\",\n    \"What light through yonder window breaks?\",\n    \"I am the king of\",\n    \"Thou art more lovely and more temperate\",\n]\n\nresults = []\n\nfor seed in seeds:\n    out = generator(seed, max_length=100, num_return_sequences=1)[0][\"generated_text\"]\n    results.append((seed, out))\n\n\n\nfor seed, gen in results:\n    print(f\"Seed: {seed}\")\n    print(f\"Generated:\\n{gen}\")\n    print(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T22:58:33.122934Z","iopub.execute_input":"2025-05-06T22:58:33.123468Z","iopub.status.idle":"2025-05-06T22:58:39.595802Z","shell.execute_reply.started":"2025-05-06T22:58:33.123446Z","shell.execute_reply":"2025-05-06T22:58:39.594921Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"Seed: To be, or not to be\nGenerated:\nTo be, or not to be; the answer is as follows:\nI do not want thy word, but to chide at\nthe thing thou hast ignorant of; being a soldier,\nI could not show him the book, or give him\na map of the place; nor wouldst give him any thing, but\nsneak'd me in my place; which I think is nothing, having\nsuch ignorance as to do either thing with\nno matter.\n\nANG\n--------------------------------------------------\nSeed: O Romeo, Romeo! wherefore art thou\nGenerated:\nO Romeo, Romeo! wherefore art thou?\n\nROMEO:\nYond, that I may kiss thee; wherefore be I?\n\nROMEO:\nNay, thou knowest me best: and wherefore art thou\nwise?\n\nJULIET:\nMarry, I know, to be no better.\n\nROMEO:\nI have but one eye more; the other is mute:\nFor I never met a sweeter ear,\n--------------------------------------------------\nSeed: The night is dark and\nGenerated:\nThe night is dark and I cannot sleep:\nLovers, I have heard, that never in the world\nHave I had a better nap than this.\n\nBENVOLIOLE:\nThis is the very first thing he told us about you;\nAnd in a short space hath he begun to think\nHe has dreamt of a happy bride; and with a little pause,\nMethinks to himself then, with a heavy heart,\nThat all the while his mind\n--------------------------------------------------\nSeed: What light through yonder window breaks?\nGenerated:\nWhat light through yonder window breaks?\nThat's one of the many wonders of this earth\nShe should be so pale as this.\nCome, take your hat. If thou lookest inest\nWith this pale image of a girl's dam,\nA bridegroom's face, o'er this land,\nA puling bawd, lookest in this picture;\nA prince, this, this, this; a king, this, this;\nA man\n--------------------------------------------------\nSeed: I am the king of\nGenerated:\nI am the king of devils, and the lord of hell.\nI cannot come to an end; then, like a beggar,\nI am your most obedient subject; and therefore\nShow me your love, which you show, not\nThe looks of beggars, who feed not beggars:\nWhy, then, come you hither, my good lord,\nTo be with you, and see what my mind\nIs to do with you. Has the devil made thee a man\n--------------------------------------------------\nSeed: Thou art more lovely and more temperate\nGenerated:\nThou art more lovely and more temperate than thy cousin's.\n\nLord:\nThou didst think no better.\n\nPAULINA:\nHilt thou not think for himself?\n\nGLOUCESTER:\nNay, but if thou dreamt that, thou wilt be content.\n\nDERBY:\nThen I'll strike thee to earth: I'll trow your face\nLest thou, poor lief, hast left my realm\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# T5","metadata":{}},{"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration\n\nmodel_name = \"t5-small\"\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel_t5 = T5ForConditionalGeneration.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T22:58:39.596850Z","iopub.execute_input":"2025-05-06T22:58:39.597131Z","iopub.status.idle":"2025-05-06T22:58:42.785827Z","shell.execute_reply.started":"2025-05-06T22:58:39.597103Z","shell.execute_reply":"2025-05-06T22:58:42.784925Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"157f822ca7e44965b4c41bb9a80bc7bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"037034d978134749a8c320e151eb75e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"469bf6aa2df547f3b8f2e50f220bbe87"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fd3677a08eb4c679d70174f93ba0f64"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2c7d3dad4e5414193e5c978beaf3f5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad19c0a6dfb5472b84797d7f08ccb1a6"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"text = \"translate English to French: The weather is nice today\"\n\ninputs = tokenizer(text, return_tensors=\"pt\", padding=True)\noutputs = model_t5.generate(**inputs, max_length=40)\n\nprint(\"Translation:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T22:58:42.786712Z","iopub.execute_input":"2025-05-06T22:58:42.787005Z","iopub.status.idle":"2025-05-06T22:58:43.054991Z","shell.execute_reply.started":"2025-05-06T22:58:42.786957Z","shell.execute_reply":"2025-05-06T22:58:43.054351Z"}},"outputs":[{"name":"stdout","text":"Translation: Le temps est agréable aujourd'hui\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"article = \"\"\"summarize: Machine learning is a method of data analysis that automates analytical model building.\nIt is a branch of artificial intelligence based on the idea that systems can learn from data,\nidentify patterns and make decisions with minimal human intervention.\"\"\"\n\ninputs = tokenizer(article, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\noutputs = model_t5.generate(**inputs, max_length=40)\n\nprint(\"Summary:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T22:58:43.055758Z","iopub.execute_input":"2025-05-06T22:58:43.055941Z","iopub.status.idle":"2025-05-06T22:58:43.588777Z","shell.execute_reply.started":"2025-05-06T22:58:43.055926Z","shell.execute_reply":"2025-05-06T22:58:43.588126Z"}},"outputs":[{"name":"stdout","text":"Summary: machine learning is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"sentences = [\n    \"translate English to German: I love natural language processing.\",\n    \"translate English to German: This model is very powerful.\",\n    \"translate English to German: Have a nice day!\"\n]\n\nfor s in sentences:\n    inputs = tokenizer(s, return_tensors=\"pt\")\n    outputs = model_t5.generate(**inputs, max_length=40)\n    print(f\"{s.split(':')[1].strip()} → {tokenizer.decode(outputs[0], skip_special_tokens=True)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T22:58:43.589485Z","iopub.execute_input":"2025-05-06T22:58:43.589702Z","iopub.status.idle":"2025-05-06T22:58:44.131366Z","shell.execute_reply.started":"2025-05-06T22:58:43.589685Z","shell.execute_reply":"2025-05-06T22:58:44.130591Z"}},"outputs":[{"name":"stdout","text":"I love natural language processing. → Ich liebe natürliche Sprachenverarbeitung.\nThis model is very powerful. → Dieses Modell ist sehr leistungsstark.\nHave a nice day! → Haben Sie einen schönen Tag!\n","output_type":"stream"}],"execution_count":10}]}